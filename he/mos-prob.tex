% !TeX root = mos-he.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\begin{center}
\textbf{\LARGE סקירה על הסתברות}
\end{center}
\addcontentsline{toc}{section}{\Large סקירה של הסתברות}

סעיף זה סוקר מושגים בהתסבתרות. אביא דוגמה של כל מושג עבור הטלת קוביה הונגת עם שש פאות. 

\textbf{ניסוי \L{\small (trial)}}
מושג לא מוגדר כאשר הכוונה היא לפעולה שיש לה תוצאות אפשריות.

\textbf{תוצאה \L{\small (outcome)}} 
התוצאה של ניסוי. אם אתה מטיל קוביה אחת תוצאה אפשרית היא 
$4$.

\textbf{מרחב מדגם \L{\small (sample space)}}
קבוצת כל התוצאות האפשריות של ניסוי. הקבוצה 
$S=\{1,2,3,4,5,6\}$
היא מרחב המדגם של הטלת קוביה.

\textbf{אירוע \L{\small (event)}}
תת-קבוצה של מרחב מדגם. תת-הקבוצה 
$e=\{2,4,6\}\subseteq S$
היא האירוע של הופעת מספר זוגי בהטלת קוביה.

\textbf{משתנה אקראי \L{\small (random variable)}}
פונקציה ממרחב המדגם למספרים. יהי 
$T$
מרחב המדגם של של הזוגות (הסדורות) הם התוצאות של הטלת זוג קוביות:
\[
T=\{(a,b)| a,b\in \{1,2,3,4,5,6\} \}\,.
\]
הגדר משתנה אקראי 
$X$
כפונקציה
$X:T \mapsto \{2,3,\ldots,11,12\}$
שממפה תוצאות של הטלת זוג קוביות לסכום המספרים על הקוביות:
\begin{equation}\label{eq.sum}
X((a,b)) = a+b\,.
\end{equation}

\textbf{איחוד, חיתוך, משלים \L{\small (union, intersection, complement)}} 
אירועים הם קבוצות ולכן למושגים הללו יש את המשמעות הרגילה בתורת הקבוצות. יהי
$e_1=\{2,4,6\}$
ו-%
$e_2=\{1,2,3\}$. 
אזי:
\[
e_1 \cup e_2=\{1,2,3,4,6\}\quad e_1 \cap e_2=\{2\}\quad \overline{e_1} = S\setminus e_1=\{1,3,5\}\,.
\]
החיתוך הוא קבוצת המספרים הזוגיים מתוך שלושת האיברים הראשונים במרחב המדגם. המשלים הוא קבוצת המספרים האי-זוגיים מתוך מרחב המדגם.

\textbf{זרים \L{\small (mutually exclusive)}} 
שני אירועים או יותר זרים זה לזה אם החיתוך שלהם הוא הקבוצה הריקה. שני נאירועים
$e_1=\{2,4,6\}$
ו-%
$e_2=\{1,3,5\}$
זרים זה לזה כי
$e_1 \cap e_2=\emptyset$,
כלומר, אין תוצאות שהן מספרים שהם גם זוגיים וגם אי-זוגיים.

\textbf{הסתברות \L{\small (probability)}}
המשמעות האינטואיטיבית של הסתברות היא שהיא הגבול של התדירות היחסית של אירוע. יהי
$e$ 
אירוע ויהי
$n_e$
מספר הפעמים שהאירוע 
$e$
מתרחש ב-%
$n$
חזרות על הניסוי. 
$P(e)$,
ההסתברות של האירוע
$e$,
היא:
\[
P(e) = \lim_{n\rightarrow \infty} \frac{n_e}{n}\,.
\]
הגדרה זו היא בעייתית כי אנחנו לא ממש יודעים אם הגבול קיים. ההגדרה תלויה על "חזרות על הניסוי" אולם אנו רוצים להגדיר הסתברות ללא קשר לסדרה מסוימת של ניסויים. 
\textbf{חוק המספרים הגדולים}
מבטיח שהתפיסה האינטואיטיבית של הסתברות כי תדירות יחסית קרא מאוד למה שקורה כאשר חוזרים על ניסוי מספר רק של פעמים.

התיאוריה המודרנית של הסתברות מבוססת על שלוש אקסיומות שהן די אינטואיטיביות:
\begin{itemize}
\item 
עבור אירוע
$e$, $P(e) \geq 0\,$.
\item 
עבור כל התוצאות האפשריות במרחת 
$S$, $P(S) = 1\,$.
\item
עבור קבוצה של אירועים זה לזה
$\{e_1,\ldots,e_n\}$:
\[
P\left(\bigcup_{i=1}^{n} e_i\right)=\sum_{i=1}^{n} P(e_i)\,.
\]
\end{itemize}

\textbf{החזרה \L{\small (replacement}}
בעיה שכיחה בהסתברות היא לשאול שאלות על שליפת כדור צבעוני מכד. חשוב שהבעיה תציין אם השליפה היא עם או בלי החזרה: לאחר שליפת הכדור האם מחזירים אותו לכד או לא לפני השליפה הבאה? נשלוף שני כדורים מכד עם שלושה כדורים אדומים ושלושה שחורים. לאירוע שהשליפה של הכדור הראשונה שולף כדור אדום הסתברות של
$\frac{3}{3+3}=\frac{1}{2}$.
אם מחזירים את הכדור לפני השליפה השניה אזי ההסתברות שהשליפה של הכדור השני שולף כדור האדום נשארת 
$\frac{1}{2}$
ולכן ההסתברות ששני הכדורים הם אדומים היא
$\frac{1}{4}$.
אם לא מחזירים את הכדור ההסתברות שהכדור השני הוא אדום יורדת ל-%
$\frac{2}{2+3}=\frac{2}{5}$,
ולכן ההסתברות ששני הכדורים הם אדומים היא
$\frac{1}{2}\cdot\frac{2}{5}=\frac{1}{4}$.

\textbf{התפלגות אחידה \L{\small (uniformly distributed)}} 
אם הסתברויות של כל התוצאת במרחב שוות להסתברות התפלגות אחידה. אם 
$S$
היא קבוצה סופית ולהסתברות שלה התפלגות אחידה אזי:
\[
P(e)=\frac{|e|}{|S|}\,.
\]
אם אתה מטיל קוביה
\textbf{הוגנת}
ההסתברות של התוצאות מתפלגת אחידה ולכן עבור
$e=\{2,4,6\}$:
\[
P(e) = \frac{|e|}{|S|} = \frac{|\{2,4,6\}|}{|\{1,2,3,4,5,6\}|}=\frac{1}{2}\,.
\]

\textbf{הסתברות מותנית \L{\small (conditional probability)}} 
יהי 
$e_1,e_2$
אירועים. 
$P(e_1 | e_2)$,
ההסתברות המותנית ש-%
$e_1$
מתרחש אם נתון ש-%
$e_2$
מתרחש, נתונה על ידי:
\[
P(e_1 | e_2) = \disfrac{P(e_1 \cap e_2)}{P(e_2)}\,.
\]
יהי 
$e_1=\{1,2,3\}$
האירוע שקוביה מראה מספר פחות או שווה ל-%
$3$
ויהי
$e_2=\{2,4,6\}$
האירוע שהקוביה מראה מספר זוגי. אזי:
\[
P(e_2 | e_1) = \disfrac{P(E_2 \cap E_1)}{P(e_1)}=\disfrac{P(\{2\})}{P(\{2,4,6\})}= \disfrac{1/6}{1/2}=\disfrac{1}{3}\,.
\]
אם אתה יודע שמספר הוא פחות או שווה ל-%
$3$,
רק אחת משלושת התוצאה היא מספר זוגי.

\textbf{בלתי-תלוי \L{\small (independence)}}
שני אירועים בלתי-תלויים אם ההסתברות של החיתוך שלהם היא המכפלה של ההסתברויות הנפרדות:
\[
P(e_1 \cap e_2)=P(e_1)\,P(e_2)\,.
\]
במונחים של הסבתרות מותנית:
\[
P(e_1 | e_2)=\frac{P(e_1)\cap P(e_2)}{P(e_2)} = \frac{P(e_1)\,P(e_2)}{P(e_2)}=P(e_1)\,. 
\]
עבור אירועים בלתי-תלויים
$e_1,e_2$,
ידיעה של הההסתברות של
$e_2$
לא מספק מידע על ההסתברות של
$e_1$.
שלוש הטלות של קוביה הוגנת בלתי-תלויות ולכן ההסתברות שכולן מראות מספר זוגי היא
$\frac{1}{2}\cdot \frac{1}{2}\cdot \frac{1}{2}=\frac{1}{8}$. 

\textbf{ממוצע \L{\small (average)}}
תהי
$S=\{a_1,\ldots,a_n\}$
קבוצה של ערכים. אזי:
\[
\mathit{Average}(S)=\disfrac{\sum_{i=1}^{n} a_i}{n}\,.
\]
ממוצע מחושב מעל לקבוצה של ערכים אבל הממוצע לא חייב להיות איבר בקבוצה. אם יש
$1000$
משפחות בעיירה ולהן
$3426$
ילדים, הממוצע של מספר הילדים למשפחה היא
$3.426$
למרות שברור שאין משפחה עם
$3.426$
ילדים. אם אתה מטיל קוביה שש פעמים ומקבל את המספרים
$\{2,2,4,4,5,6\}$
הממוצע הוא:
\[
\frac{2+2+4+4+5+6}{6}=\frac{23}{6}\approx 3.8\,,
\]
שוב, לא איבר בקבוצה.

\textbf{תוחלת \L{\small (expectation)}}
התוחלת של משתנה אקראי היא סכום ההסתברויות של כל תוצאה כפול הערך של משתנה האקראי עבור אותה תוצאה. עבור קוביה הוגנת לכל תוצאה יש הסתברות זהה ולכן:
\[
E(\textrm{קוביה ערך})=1\cdot \frac{1}{6} + 2\cdot\frac{1}{6} + 3\cdot\frac{1}{6} + 4\cdot\frac{1}{6} + 5\cdot\frac{1}{6} + 6\cdot\frac{1}{6}=3.5\,.
\]
השתנה האקראי
$X$
ממשוואה%
~\ref{eq.sum}
ממפה את המספרים המופיעים על זוג קוביות לסכום המספרים. ההסתברות של כל זוג היא
$1/36$,
אבל לזוגות
$(2,5)$
ו-%
$(5,2)$
אותו סכום ולכן הם שייכים לאותה תוצאה. הערכים של המשתנה האקראי הם
$\{2,\ldots,12\}$
ומספר הדרכים לקבל כל אחד מהן הם:
\[
\begin{array}{l|rrrrrrrrrrr}
\textrm{סכום} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\\hline
\textrm{זוגות} & 1 & 2 & 3 & 4 & 5 & 6 & 5 & 4 & 3 & 2 & 1
\end{array}
\]
התוחלת היא הממוצע של ערכי המשתנה האקראי כפול 
\textbf{המשקל}
שהוא ההסתברות של כל תוצאה. יהי
$E_s$
התוחלת של סכום הערכים כאשר מטילים זוג קוביות. אזי:
\begin{equation}\label{eq.two-dice}
E_s=2\cdot \frac{1}{36} + 3\cdot \frac{2}{36} + 4\cdot \frac{3}{36} + 
\cdots + 10\cdot \frac{3}{36} + 11\cdot \frac{2}{36} + 12\cdot \frac{1}{36} = 7\,.
\end{equation}
עבור קבוצה שרירותית של אירועים
$\{e_1,\ldots,e_n\}$
התוחלת היא:
\[
E=\sum_{i=1}^{n} e_iP(e_i)\,.
\]

\textbf{ליניאריות של התוחלת \L{\small (linearity of expectation)}}\label{p.linearity}
נעיין שוב בתוחלת של הסכום של זוג קוביות (משוואה%
~\ref{eq.two-dice}).
תהי 
$E(e_6)$
התוחלת של האירוע שסכום הקוביות הוא
$6$.
אזי:
\[
E(e_6) = X(e_6)P(e_6)=6\cdot \frac{5}{36}\,,
\]
כי יש
$5$
זוגות מתוך 
$36$
הזוגות האפשריים שסכומם 
$6$: $(1,5), (2,4), (3,3), (4,2), (5,1)$.
אבל ניתן לחשב את התוחלת כדלקמן כאשר
$P(i,j)$
היא ההסתברות שהזוג 
$(i,j)$
מופיע ו-%
$E(i,j)$
היא התוחלת של
$i+j$:
\begin{eqn}
E(X(e_6)) &=& 6\cdot P(1,5)+6\cdot P(2,4)+6\cdot P(3,3)+6\cdot P(4,2)+6\cdot P(5,1)\\
&=& (1+5)\cdot \textstyle\frac{1}{36}+(2+4)\cdot \frac{1}{36}+(3+3)\cdot \frac{1}{36}+(4+2)\cdot \frac{1}{36} +(5+1)\frac{1}{36}\\
&=&E(1,5)+E(2,4)+E(3,3)+E(4,2)+E(5,1)\\
&=&\sum_{i,j | i+j=6}E(i,j)\,.
\end{eqn}
החישוב תלוי בעובדה שהאירועים בלתי-תלויים כי ברור ש-%
$(2,4)$
ו-%
$(3,3)$
לא יכולים להופיע באותו ניסוי.

באותה שיטה ניתן להוכיח את ההכללה
(\L{\cite[Section~4.9]{ross}}):
\[
E\left(\sum_{i=1}^{n} a_ie_i\right)=\sum_{i=1}^{n} a_iE(e_i)\,,
\]
שנקראת הליניאריות של התוחלת. במקרה של שני משתנים אקראיים:
\[
E(ae_1 + be_2) = aE(e_1) + bE(e_2)\,.
\]

\textbf{משתנה מסמן \L{\small (indicator variable)}}
יהי
$e$
אירוע שההסתברות שלה היא
$P(e)$.
הגדר
$I_e$,
משתנה מסמן עבור 
$e$,
כך
\L{\cite[Chapter~4, Example~3b]{ross}}:
\[
I_e=
\left\{
\begin{array}{ll}
1,\quad \textrm{מתרחש}\; e\;\textrm{אם}\\
0, \quad \textrm{מתרחש לא}\;e\;\textrm{אם}\,.
\end{array}
\right.
\]
מכאן ש:
\[
E(I_e)=1\cdot P(e) + 0\cdot (1-P(e))=P(e)\,.
\]
ניתן להכליל משוואה זו. נתונה קבוצה של אירועים
$\{e_1,\ldots\}$
והמשתנים המסמנים שלהם
$\{I_1,\ldots\}$:
\begin{equation}\label{eq.expectation-prob}
E\left(\sum_{i=1}^{\infty} I_{i}\right) = \sum_{i=1}^{\infty} I_{i}p(e_i) = \sum_{i=1}^{\infty} (1\cdot p(e_i) + 0\cdot (1-p(e_i)) =\sum_{i=1}^{\infty} p(e_i)\,.
\end{equation}
בנוסף:
\begin{equation}\label{eq.expectation-sum}
\sum_{i=1}^{\infty} E(I_{i})=E\left(\sum_{i=1}^{\infty} I_{i}\right)\,.
\end{equation}
ההוכחה של נוסחה זו קשה והנוסחה תקיפה כאן בגלל שהמשתנים באקראיים לא שליליים.

\textbf{משפט הבינום \L{\small (binomial theorem)}}
אם
$p$
היא ההסתברות של אירוע
$e$
אזי ההסתברות שהתוצאה של סדרה של 
$n$
ניסויים בלתי-תלויים היא
\textbf{בדיוק}
$k$
אירועים 
$e$
ניתנת על ידי
\textbf{המקדם הבינומי (\L{binomial coefficient})}:
\[
\dischoose{n}{k} p^k (1-p)^{n-k}\,.
\]
בהכללה, ההסתברות ש-%
$e$
מתרחוש בין
$i$
ל-%
$j$
פעמים היא:
\[
\sum_{k=i}^{j}\dischoose{n}{k} p^k (1-p)^{n-k}\,.
\]
לפי משפט הבינום:
\begin{eqn}
\sum_{i=0}^{n} \dischoose{n}{i} x^i y^{n-i}&=&(x+y)^n\\
\sum_{i=0}^{n} \dischoose{n}{i} p^i (1-p)^{n-i}&=&(1+(1-p))^n=1\,,
\end{eqn}

כפי שאפשר לצפות כי אחת התוצאות חייבת להתרחש.

\textbf{סכום סדרה הרמונית \L{\small (sum of a harmonic series)}}\label{p.harmonic}
עבור 
$n$
מספר שלם חיובי, הסדרה ההרמונית היא:
\[
H_n=\sum_{k=1}^{n}\disfrac{1}{k}\approx \ln n + \frac{1}{2n} + \gamma\,,
\]
כאשר
$\gamma \approx 0.5772$
הוא
\textbf{הקבוע של \L{Euler} \L{(Euler's constant)}}.
כאשר
$n$
שואף לאינסוף הסדרה מתבדרת:
\[
\sum_{k=1}^{\infty}\disfrac{1}{k}=\infty\,,
\]
כי
$\ln n$
אינו חסום.

\textbf{הקירוב של \L{Stirling} \L{\small (Stirling's approximation)}}
קשה מאוד לחשב
$n!$ 
עבור 
$n$ 
גדול. נוח להשתמש באחת הנוסחאות של הקירוב של
\L{Stirling}:
\begin{eqn}
n! &\approx& \sqrt{2\pi n}\left(\disfrac{n}{e}\right)^n\\
\ln (n!) &\approx& n\ln n - n\\
\ln (n!)  &\approx& n\ln n - n + \frac{1}{6}\left(8n^3+4n^2+n+\frac{1}{30}\right)+\frac{1}{2}\ln\pi\,.
\end{eqn}

\textbf{התפלגות הסתברותית רציפה\L{\small (Continuous probability distribution)}}\label{p.continuous}
התפלגויות הסתברות רציפות בדרך כלל לא מופיעות בספר אבל עבור קוראים עם הרקע המתאים אנו סורקים את המושגים הבסיסיים.

ניתן להגדיר הסתברויות מעל למשתנים אקראים רציפים.  
\textbf{פונקציית הסתברות צפיפות \L{\small (probability density function (PDF))}} $f(x): \mathcal{R}\rightarrow \mathcal{R}$
ממפה תוצאה 
$x$
לערך של הפונקציה וכך להגדיר:
\[
P(x) = f(x)\,.
\]
הסיבות למונח זו היא שההסתברות של ההופעה של כל מספר ממשי
\textbf{בודד}
היא אפס, ולכן הדרך הנכונה היא לתת הסתברויות לקטעים:
\[
P(a<x<b) = \int_{a}^{b} f(x)\, dx\,.
\]
האינטגרל הוא גם $P(a\leq x\leq b)$ כי ההסתברות של נקודות בודדות היא אפס. 

כמו כל הגדרה של הסתברות,
$P(x)\geq 0$
לכל
$x$,
וכן:
\[
\int_{-\infty}^{\infty} P(x)\, dx=\int_{-\infty}^{\infty} f(x)=1\, dx\,.
\]
אם ערכו של האיטגרל אינו
$1$
חייבים להשתמש 
\textbf{בקבוע נירמול \L{\small (normalization constant)}}.
אם ה-%
PDF
מתפלגת אחידה בקטע
$[a,b]$
אזי:
\[
P(a\leq x \leq b)=\int_{a}^{b} 1\, dx=(b-a)\,,
\]
ולכן חייבים להגדיר:
\[
P(a\leq x \leq b)=\disfrac{1}{b-a}\int_{a}^{b} 1\, dx=\disfrac{1}{b-a}\cdot (b-a)=1\,.
\]
ניתן לחשב את התוחלת על ידי אינטגרציה של ה-PDF
$f(x)$
כפול
$x$:
\[
E(x)=\int_{-\infty}^{\infty} xf(x)\, dx\,.
\]

\textbf{התפלגות הסתברות מצטברת \L{\small (cumulative probability distribution (CPD))}}
עבור הקטע
$[-\infty,a]$
מתקבלת על ידי אינטגרציה של ה-PDF:
\[
P(x<a) = \int_{-\infty}^{a} f(x)\, dx\,.
\]
ניתן לקבל את ה-PDF על ידי גזירה של ה-CPD:
\[
P(x<a)= \frac{d}{da}\mathit{CDP}(x<a)\,.
\]
